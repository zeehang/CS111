README for Lab2B

Question 2.3.1 - Cycles in the basic implementation
Where do you believe most of the cycles are spent in the 1 and 2-thread tests (for both add and list)?  Why do you believe these to be the most expensive parts of the code?

    Because there are few threads, not a lot of time will be spent waiting for locks. That means that the add
    operations can proceed almost as soon as they've been called. That means with a low number of threads
    the number of operations per second will be at its highest, which renders the most CPU usage, therefore being expensive.

Where do you believe most of the time/cycles are being spent in the high-thread spin-lock tests?
Where do you believe most of the time/cycles are being spent in the high-thread mutex tests?

    As the number of threads increases, more threads will be competing for the same amount of CPU time. Therefore,
    the thoroughput per second will decrease, because more time will simply be spent waiting for other threads
    to relinquish control of the lock. Additionally the overhead required to switch between threads will also
    take up more time. Therefore, most of the time spent in both the highthread spin-lock and mutex tests
    will be spent waiting for the lock to become available.

Question 2.3.2 - Execution profiling
Where (what lines of code) are consuming most of the cycles when the spin-lock version of the list exerciser is run with a large number of threads?

    The large majority of the time spent is in the spin lock while loop.

Why does this operation become so expensive with large numbers of threads?

    As the number of threads used increased, there will be more threads competing for locks.
    Therefore, there's a greater chance that the lock is already in use, meaning that the thread will continue to spin
    until it is available.

Question 2.3.3 - Mutex wait time
Why does the average lock-wait time rise so dramatically with the number of contending threads?

    The more threads that are competing, the more likely that when a thread encounter a lock, that the lock will be in use.
    Additionally, when the lock does become available, the more likely it is that there will be a "queue" of threads
    waiting for that now open lock. Therefore, lock-wait time increases a lot with more threads

Why does the completion time per operation rise (less dramatically) with the
number of contending threads?

    The more time that is spent waiting for locks, the longer completion time will take. However, this line
    rises less dramatically than the lock time because the time to complete the actual operation (insert, lookup delete)
    remains constant for each thread.

How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?

    The total completion time meausres only the parent thread, while *each thread* measures its lock time (e.g. you can have
    multiple threads waiting at the same time and adding to the total). Essentially, there can be multiple threads (running 
    in parallel at the same time) which add to the wait time total - which means it can be greater than completion time.

Question 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods as a function of the number of lists.

    With both spin and mutex lock, the performance increases as more lists are created.

Should the throughput continue increasing as the number of lists is further increased?  If not, explain why not.

    We can see that the lines with the larger number of lists tend to level off with the highe rnumber of threads.
    Once the number of lists is equivalent or greater to the number of threads, there will be a very low risk of conflict,
    so additional lists will have no large impact or improvement in performance.

It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads.  Does this appear to be true in the above curves?  If not, explain why not.

    This is not true. Having less threads simply leads to less possibility of conflict/wait time between each thread. While lists 
    do lessen wait time, they also provide additional benefits. Because each sublist is shorter, inserting and looking up elements
    should be shorter, since you'll be traversing through less elements.


Files contained:
    SortedList.c - multithreaded implementation of a doubly linked list with insert, delete, lookup and length functions
    SortedList.h - header for the .c file
    lab2b.gp - used with gnuplot to generate the graphs
    lab2_list.csv and lab2_add.csv are included to create the first graph
    .png files - graphs generated by gnuplot
        lab2b_1.png - shows throughput vs number of threads, comparing mutex and spin for both add and list operations
        lab2b_2.png - shows mean time per mutex wait and mean time per operation
        lab2b_3.png - Shows succesful iterations for synchronization methods
        lab2b_4.png - compares performance of a paritioned list for spinlock operations
        lab2b_4.png - compares performance of a paritioned list for mutex operations
    Makefile - contains options to make the files and also create graphs and run tests
    .lab_2b_list.csv - contains all the files generated by the make tests option 
    profile.gperf is generated by the make profile operation and shows performance for the list operations with spinlock



pprof results

Total: 535 samples
     499  93.3%  93.3%      535 100.0% newthread
      26   4.9%  98.1%       26   4.9% __strcmp_sse42
       5   0.9%  99.1%       17   3.2% SortedList_lookup
       4   0.7%  99.8%       19   3.6% SortedList_insert
       1   0.2% 100.0%        1   0.2% _init
       0   0.0% 100.0%      535 100.0% __clone
       0   0.0% 100.0%      535 100.0% start_thread
ROUTINE ====================== newthread in /u/cs/ugrad/zhangm/CS111/lab2/lab2a/lab2btest/lab2_list.c
   499    535 Total samples (flat / cumulative)
     .      .  130:         
     .      .  131:     }
     .      .  132: }
     .      .  133: 
     .      .  134: void* newthread(void *arrset)
---
     .      .  135: {
     .      .  136:     int* setnum = (int*)arrset;
     .      .  137:     struct timespec lockbegin, lockend;
     .      .  138:    // fprintf(stdout, "Which iteration: %d\n", *setnum);
     .      .  139:     
     .      .  140:     for (int i = (*setnum)*iterations; i < ((*setnum)+1)*(iterations); i++)
     .      .  141:     {
     .      .  142:         //SortedListElement_t* elementtoadd = elementarr[i];
     .      .  143:         SortedListElement_t* elementptr = &elementarr[i];
     .      .  144:         if(syncmflag)
     .      .  145:         {
     .      .  146:             clock_gettime(CLOCK_MONOTONIC, &lockbegin);
     .      .  147:             pthread_mutex_lock(&mut);
     .      .  148:             clock_gettime(CLOCK_MONOTONIC, &lockend);
     .      .  149:             long int elapsedntime = 1000000000L*(lockend.tv_sec - lockbegin.tv_sec) + (lockend.tv_nsec - lockbegin.tv_nsec);
     .      .  150:             totallocktime += elapsedntime;
     .      .  151:          }
     .      .  152:         if(syncsflag)
   291    291  153:             while(__sync_lock_test_and_set(&spin_lock, 1));
     .      .  154:        // fprintf(stdout, "Element ptr %p\n", elementptr);
     .     19  155:         SortedList_insert(list, elementptr);
     .      .  156:         if(syncmflag)
     .      .  157:             pthread_mutex_unlock(&mut);
     .      .  158:         if(syncsflag)
     .      .  159:             __sync_lock_release(&spin_lock);
     .      .  160:     }
     .      .  161:     int listlength;
     .      .  162:     listlength = SortedList_length(list);
     .      .  163:     if(listlength == 0)
     .      .  164:     {
     .      .  165:         fprintf(stderr, "No list elements inserted! \n");
     .      .  166:         exit(1);
     .      .  167:     }
     .      .  168:     for (int i = (*setnum)*iterations; i < ((*setnum)+1)*(iterations); i++)
     .      .  169:     {
     .      .  170:       //  SortedListElement_t* elementtoremove = elementarr[i];
     .      .  171:        // fprintf(stdout, "Deleting this element: %d\n", i);
     .      .  172:        //look up before deleting
     .      .  173:        const char* key = elementarr[i].key;
     .      .  174:        if(syncmflag)
     .      .  175:         {
     .      .  176:             clock_gettime(CLOCK_MONOTONIC, &lockbegin);
     .      .  177:             pthread_mutex_lock(&mut);
     .      .  178:             clock_gettime(CLOCK_MONOTONIC, &lockend);
     .      .  179:             long int elapsedntime = 1000000000L*(lockend.tv_sec - lockbegin.tv_sec) + (lockend.tv_nsec - lockbegin.tv_nsec);
     .      .  180:             totallocktime += elapsedntime;
     .      .  181:          }
     .      .  182:          if(syncsflag)
   201    201  183:              while(__sync_lock_test_and_set(&spin_lock, 1));
     .     17  184:        SortedList_lookup(list, key);
     .      .  185:      if(syncmflag)
     .      .  186:         pthread_mutex_unlock(&mut);
     .      .  187:     if(syncsflag)
     .      .  188:         __sync_lock_release(&spin_lock);
     .      .  189:     
     .      .  190:         if(syncmflag)
     .      .  191:         {
     .      .  192:             clock_gettime(CLOCK_MONOTONIC, &lockbegin);
     .      .  193:             pthread_mutex_lock(&mut);
     .      .  194:             clock_gettime(CLOCK_MONOTONIC, &lockend);
     .      .  195:             long int elapsedntime = 1000000000L*(lockend.tv_sec - lockbegin.tv_sec) + (lockend.tv_nsec - lockbegin.tv_nsec);
     .      .  196:             totallocktime += elapsedntime;
     .      .  197:          }
     .      .  198:          if(syncsflag)
     7      7  199:              while(__sync_lock_test_and_set(&spin_lock, 1));
     .      .  200:         
     .      .  201:         if(SortedList_delete(&elementarr[i]))
     .      .  202:         {
     .      .  203:             if(syncmflag)
     .      .  204:                 pthread_mutex_unlock(&mut);
     .      .  205:             if(syncsflag)
     .      .  206:                  __sync_lock_release(&spin_lock);
     .      .  207:             exit(1);
     .      .  208:         }
     .      .  209:             if(syncmflag)
     .      .  210:         pthread_mutex_unlock(&mut);
     .      .  211:     if(syncsflag)
     .      .  212:         __sync_lock_release(&spin_lock);
     .      .  213:     }
     .      .  214: 
     .      .  215: 
     .      .  216:     // if(listlength)
     .      .  217:     // {
     .      .  218:     //     fprintf(stderr, "Ending list length is not zero! \n");
     .      .  219:     //     exit(1);
     .      .  220:     // }
     .      .  221: }
---
     .      .  222: 
     .      .  223: void read_yield_args(char* optarg)
     .      .  224: {
     .      .  225:    // fprintf(stdout, "%c", '-');
     .      .  226:    // fprintf(stdout, "Legnth of yield: %d", strlen(optarg));
ROUTINE ====================== newthread in /u/cs/ugrad/zhangm/CS111/lab2/lab2a/lab2btest/lab2_list.c
   499    535 Total samples (flat / cumulative)
     .      .  130:         
     .      .  131:     }
     .      .  132: }
     .      .  133: 
     .      .  134: void* newthread(void *arrset)
---
     .      .  135: {
     .      .  136:     int* setnum = (int*)arrset;
     .      .  137:     struct timespec lockbegin, lockend;
     .      .  138:    // fprintf(stdout, "Which iteration: %d\n", *setnum);
     .      .  139:     
     .      .  140:     for (int i = (*setnum)*iterations; i < ((*setnum)+1)*(iterations); i++)
     .      .  141:     {
     .      .  142:         //SortedListElement_t* elementtoadd = elementarr[i];
     .      .  143:         SortedListElement_t* elementptr = &elementarr[i];
     .      .  144:         if(syncmflag)
     .      .  145:         {
     .      .  146:             clock_gettime(CLOCK_MONOTONIC, &lockbegin);
     .      .  147:             pthread_mutex_lock(&mut);
     .      .  148:             clock_gettime(CLOCK_MONOTONIC, &lockend);
     .      .  149:             long int elapsedntime = 1000000000L*(lockend.tv_sec - lockbegin.tv_sec) + (lockend.tv_nsec - lockbegin.tv_nsec);
     .      .  150:             totallocktime += elapsedntime;
     .      .  151:          }
     .      .  152:         if(syncsflag)
   291    291  153:             while(__sync_lock_test_and_set(&spin_lock, 1));
     .      .  154:        // fprintf(stdout, "Element ptr %p\n", elementptr);
     .     19  155:         SortedList_insert(list, elementptr);
     .      .  156:         if(syncmflag)
     .      .  157:             pthread_mutex_unlock(&mut);
     .      .  158:         if(syncsflag)
     .      .  159:             __sync_lock_release(&spin_lock);
     .      .  160:     }
     .      .  161:     int listlength;
     .      .  162:     listlength = SortedList_length(list);
     .      .  163:     if(listlength == 0)
     .      .  164:     {
     .      .  165:         fprintf(stderr, "No list elements inserted! \n");
     .      .  166:         exit(1);
     .      .  167:     }
     .      .  168:     for (int i = (*setnum)*iterations; i < ((*setnum)+1)*(iterations); i++)
     .      .  169:     {
     .      .  170:       //  SortedListElement_t* elementtoremove = elementarr[i];
     .      .  171:        // fprintf(stdout, "Deleting this element: %d\n", i);
     .      .  172:        //look up before deleting
     .      .  173:        const char* key = elementarr[i].key;
     .      .  174:        if(syncmflag)
     .      .  175:         {
     .      .  176:             clock_gettime(CLOCK_MONOTONIC, &lockbegin);
     .      .  177:             pthread_mutex_lock(&mut);
     .      .  178:             clock_gettime(CLOCK_MONOTONIC, &lockend);
     .      .  179:             long int elapsedntime = 1000000000L*(lockend.tv_sec - lockbegin.tv_sec) + (lockend.tv_nsec - lockbegin.tv_nsec);
     .      .  180:             totallocktime += elapsedntime;
     .      .  181:          }
     .      .  182:          if(syncsflag)
   201    201  183:              while(__sync_lock_test_and_set(&spin_lock, 1));
     .     17  184:        SortedList_lookup(list, key);
     .      .  185:      if(syncmflag)
     .      .  186:         pthread_mutex_unlock(&mut);
     .      .  187:     if(syncsflag)
     .      .  188:         __sync_lock_release(&spin_lock);
     .      .  189:     
     .      .  190:         if(syncmflag)
     .      .  191:         {
     .      .  192:             clock_gettime(CLOCK_MONOTONIC, &lockbegin);
     .      .  193:             pthread_mutex_lock(&mut);
     .      .  194:             clock_gettime(CLOCK_MONOTONIC, &lockend);
     .      .  195:             long int elapsedntime = 1000000000L*(lockend.tv_sec - lockbegin.tv_sec) + (lockend.tv_nsec - lockbegin.tv_nsec);
     .      .  196:             totallocktime += elapsedntime;
     .      .  197:          }
     .      .  198:          if(syncsflag)
     7      7  199:              while(__sync_lock_test_and_set(&spin_lock, 1));
     .      .  200:         
     .      .  201:         if(SortedList_delete(&elementarr[i]))
     .      .  202:         {
     .      .  203:             if(syncmflag)
     .      .  204:                 pthread_mutex_unlock(&mut);
     .      .  205:             if(syncsflag)
     .      .  206:                  __sync_lock_release(&spin_lock);
     .      .  207:             exit(1);
     .      .  208:         }
     .      .  209:             if(syncmflag)
     .      .  210:         pthread_mutex_unlock(&mut);
     .      .  211:     if(syncsflag)
     .      .  212:         __sync_lock_release(&spin_lock);
     .      .  213:     }
     .      .  214: 
     .      .  215: 
     .      .  216:     // if(listlength)
     .      .  217:     // {
     .      .  218:     //     fprintf(stderr, "Ending list length is not zero! \n");
     .      .  219:     //     exit(1);
     .      .  220:     // }
     .      .  221: }
---
     .      .  222: 
     .      .  223: void read_yield_args(char* optarg)
     .      .  224: {
     .      .  225:    // fprintf(stdout, "%c", '-');
     .      .  226:    // fprintf(stdout, "Legnth of yield: %d", strlen(optarg));
